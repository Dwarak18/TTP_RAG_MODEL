{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5efdb8f2",
      "metadata": {
        "id": "5efdb8f2"
      },
      "source": [
        "# Security TTP RAG Model Training\n",
        "\n",
        "This notebook demonstrates how to train a Retrieval-Augmented Generation (RAG) model using the `tumeteor/Security-TTP-Mapping` dataset for cybersecurity Tactics, Techniques, and Procedures (TTP) analysis.\n",
        "\n",
        "## Overview\n",
        "- **Dataset**: Security-TTP-Mapping from Hugging Face\n",
        "- **Model Type**: RAG (Retrieval-Augmented Generation)\n",
        "- **Use Case**: Security knowledge base for TTP analysis and Q&A\n",
        "- **Components**: Vector database, embedding model, and language model\n",
        "\n",
        "## Setup Requirements\n",
        "Make sure you have installed all required dependencies from `requirements.txt`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5906e0cf",
      "metadata": {
        "id": "5906e0cf"
      },
      "source": [
        "## 1. Import Required Libraries\n",
        "\n",
        "Import all necessary libraries for RAG model training including datasets, transformers, and vector database tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "fb95e07e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb95e07e",
        "outputId": "5796e5a9-12b8-4413-db21-fdfc2fe71ec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All libraries imported successfully!\n",
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n"
          ]
        }
      ],
      "source": [
        "# Core libraries\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Dataset and ML libraries\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    TrainingArguments, Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "\n",
        "# Embedding and vector database\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "import faiss\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "39a0645e",
        "outputId": "491044d6-88b9-499d-8db4-35291c36484b"
      },
      "source": [
        "%pip install faiss-cpu"
      ],
      "id": "39a0645e",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5c28a352",
        "outputId": "192b98dc-0ca2-40b4-d218-fb6b860c6c62"
      },
      "source": [
        "%pip install chromadb"
      ],
      "id": "5c28a352",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.0.20)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.7)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.14.1)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.22.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.21.4)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.2)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.36.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.7)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0fe60c4",
      "metadata": {
        "id": "e0fe60c4"
      },
      "source": [
        "## 2. Load and Explore the Dataset\n",
        "\n",
        "Load the Security-TTP-Mapping dataset from Hugging Face and explore its structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b842a641",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b842a641",
        "outputId": "62aff25a-38a9-4bd9-b853-0e606a2ebaac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Security-TTP-Mapping dataset...\n",
            "Dataset loaded successfully!\n",
            "Available splits: ['train', 'validation', 'test']\n",
            "\n",
            "=== TRAIN SPLIT ===\n",
            "Number of examples: 14936\n",
            "Features: ['text1', 'labels']\n",
            "\n",
            "Sample data:\n",
            "  text1: The command processing function starts by substituting the main module name and path in the hosting ...\n",
            "  labels: ['T1057']\n",
            "\n",
            "=== VALIDATION SPLIT ===\n",
            "Number of examples: 2630\n",
            "Features: ['text1', 'labels']\n",
            "\n",
            "Sample data:\n",
            "  text1: Remexi boasts features that allow it to gather keystrokes, take screenshots of windows of interest (...\n",
            "  labels: ['T1056.001', 'T1113']\n",
            "\n",
            "=== TEST SPLIT ===\n",
            "Number of examples: 3170\n",
            "Features: ['text1', 'labels']\n",
            "\n",
            "Sample data:\n",
            "  text1: The spear phishing emails contained three attachments in total, each of which exploited an older vul...\n",
            "  labels: ['T1203']\n"
          ]
        }
      ],
      "source": [
        "# Load the Security TTP Mapping dataset\n",
        "print(\"Loading Security-TTP-Mapping dataset...\")\n",
        "ds = load_dataset(\"tumeteor/Security-TTP-Mapping\")\n",
        "\n",
        "print(f\"Dataset loaded successfully!\")\n",
        "print(f\"Available splits: {list(ds.keys())}\")\n",
        "\n",
        "# Explore dataset structure\n",
        "for split_name, split_data in ds.items():\n",
        "    print(f\"\\n=== {split_name.upper()} SPLIT ===\")\n",
        "    print(f\"Number of examples: {len(split_data)}\")\n",
        "\n",
        "    if len(split_data) > 0:\n",
        "        sample = split_data[0]\n",
        "        print(f\"Features: {list(sample.keys())}\")\n",
        "        print(\"\\nSample data:\")\n",
        "        for key, value in sample.items():\n",
        "            if isinstance(value, str):\n",
        "                print(f\"  {key}: {value[:100]}...\" if len(value) > 100 else f\"  {key}: {value}\")\n",
        "            else:\n",
        "                print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "7d9e53eb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d9e53eb",
        "outputId": "bfa82c6d-7ef8-4a30-f1b0-beb5542c64e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (14936, 2)\n",
            "\n",
            "Column information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14936 entries, 0 to 14935\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   text1   14936 non-null  object\n",
            " 1   labels  14936 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 233.5+ KB\n",
            "None\n",
            "\n",
            "First 3 rows:\n",
            "                                               text1         labels\n",
            "0  The command processing function starts by subs...      ['T1057']\n",
            "1  Along the way, HermeticWiper’s more mundane op...  ['T1569.002']\n",
            "2  These Microsoft Office templates are hosted on...  ['T1584.004']\n",
            "\n",
            "Text column statistics:\n",
            "text1: min=7, max=2502, mean=164.5\n",
            "labels: min=9, max=100, mean=11.9\n"
          ]
        }
      ],
      "source": [
        "# Convert to pandas for easier analysis\n",
        "if 'train' in ds:\n",
        "    df = ds['train'].to_pandas()\n",
        "else:\n",
        "    # Use the first available split\n",
        "    first_split = list(ds.keys())[0]\n",
        "    df = ds[first_split].to_pandas()\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nColumn information:\")\n",
        "print(df.info())\n",
        "\n",
        "print(f\"\\nFirst 3 rows:\")\n",
        "print(df.head(3))\n",
        "\n",
        "# Analyze text lengths if there are text columns\n",
        "text_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
        "if text_columns:\n",
        "    print(f\"\\nText column statistics:\")\n",
        "    for col in text_columns:\n",
        "        if df[col].notna().any():\n",
        "            lengths = df[col].dropna().str.len()\n",
        "            print(f\"{col}: min={lengths.min()}, max={lengths.max()}, mean={lengths.mean():.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8543eab",
      "metadata": {
        "id": "c8543eab"
      },
      "source": [
        "## 3. Preprocess the Data\n",
        "\n",
        "Clean and preprocess the security TTP data for RAG training. This includes text cleaning, chunking, and preparation for embedding generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "35c61f86",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35c61f86",
        "outputId": "18d1bf96-8149-4247-fc45-eb79dbe0570f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing dataset...\n",
            "Created 15068 text chunks from 14936 original examples\n",
            "\n",
            "Sample processed data:\n",
            "\n",
            "Chunk 1:\n",
            "ID: 0_0\n",
            "Text: The command processing function starts by substituting the main module name and path in the hosting process PEB, with the one of the default internet ...\n",
            "Metadata: {'labels': \"['T1057']\"}\n",
            "\n",
            "Chunk 2:\n",
            "ID: 1_0\n",
            "Text: Along the way, HermeticWiper’s more mundane operations provide us with further IOCs to monitor for. These include the momentary creation of the abused...\n",
            "Metadata: {'labels': \"['T1569.002']\"}\n",
            "\n",
            "Chunk 3:\n",
            "ID: 2_0\n",
            "Text: These Microsoft Office templates are hosted on a command and control server and the downloaded link is embedded in the first stage malicious document...\n",
            "Metadata: {'labels': \"['T1584.004']\"}\n"
          ]
        }
      ],
      "source": [
        "def create_text_chunks(text: str, chunk_size: int = 256, chunk_overlap: int = 50) -> List[str]:\n",
        "    \"\"\"Split text into overlapping chunks for better retrieval\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "\n",
        "    for i in range(0, len(words), chunk_size - chunk_overlap):\n",
        "        chunk = ' '.join(words[i:i + chunk_size])\n",
        "        if len(chunk.strip()) > 0:\n",
        "            chunks.append(chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def preprocess_dataset(df: pd.DataFrame) -> List[Dict]:\n",
        "    \"\"\"Preprocess the dataset for RAG training\"\"\"\n",
        "    processed_data = []\n",
        "\n",
        "    for idx, row in df.iterrows():\n",
        "        try:\n",
        "            # Create comprehensive text representation\n",
        "            text_parts = []\n",
        "            context_parts = []\n",
        "\n",
        "            for col, value in row.items():\n",
        "                if pd.notna(value) and isinstance(value, str) and len(value.strip()) > 0:\n",
        "                    text_parts.append(f\"{col}: {value}\")\n",
        "\n",
        "                    # Identify context-relevant fields\n",
        "                    if col.lower() in ['description', 'technique', 'procedure', 'detail', 'content', 'text', 'text1']: # Added 'text1'\n",
        "                        context_parts.append(value)\n",
        "\n",
        "            full_text = \" | \".join(text_parts)\n",
        "            context_text = \" \".join(context_parts) if context_parts else full_text\n",
        "\n",
        "            # Create chunks\n",
        "            chunks = create_text_chunks(context_text)\n",
        "\n",
        "            for chunk_idx, chunk in enumerate(chunks):\n",
        "                metadata = {k: v for k, v in row.items() if pd.notna(v) and (not isinstance(v, str) or len(str(v)) < 100)}\n",
        "                # Ensure metadata is not empty\n",
        "                if metadata:\n",
        "                    processed_item = {\n",
        "                        'id': f\"{idx}_{chunk_idx}\",\n",
        "                        'original_id': idx,\n",
        "                        'text': chunk,\n",
        "                        'full_context': full_text,\n",
        "                        'metadata': metadata\n",
        "                    }\n",
        "                    processed_data.append(processed_item)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing row {idx}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "# Preprocess the data\n",
        "print(\"Preprocessing dataset...\")\n",
        "processed_data = preprocess_dataset(df)\n",
        "print(f\"Created {len(processed_data)} text chunks from {len(df)} original examples\")\n",
        "\n",
        "# Show sample processed data\n",
        "print(\"\\nSample processed data:\")\n",
        "for i, item in enumerate(processed_data[:3]):\n",
        "    print(f\"\\nChunk {i+1}:\")\n",
        "    print(f\"ID: {item['id']}\")\n",
        "    print(f\"Text: {item['text'][:150]}...\")\n",
        "    print(f\"Metadata: {item['metadata']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c81fd09",
      "metadata": {
        "id": "5c81fd09"
      },
      "source": [
        "## 4. Setup Vector Database\n",
        "\n",
        "Initialize ChromaDB for efficient similarity search and document retrieval in our RAG system."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "e6cd4777",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6cd4777",
        "outputId": "c3ae4a25-39e4-4c45-eec8-505287d0bab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up ChromaDB...\n",
            "Collection 'security_ttp' ready for use!\n"
          ]
        }
      ],
      "source": [
        "# Setup ChromaDB for vector storage\n",
        "print(\"Setting up ChromaDB...\")\n",
        "\n",
        "# Create ChromaDB client\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# Create or get collection\n",
        "collection_name = \"security_ttp\"\n",
        "try:\n",
        "    collection = chroma_client.get_or_create_collection(name=collection_name)\n",
        "    print(f\"Collection '{collection_name}' ready for use!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error setting up ChromaDB collection: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84978743",
      "metadata": {
        "id": "84978743"
      },
      "source": [
        "## 5. Create Document Embeddings\n",
        "\n",
        "Generate embeddings for all security documents using SentenceTransformers and store them in the vector database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "038f5656",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "038f5656",
        "outputId": "ba604f24-1e82-49e6-d2f8-3a85097289f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model...\n",
            "Embedding model loaded successfully!\n",
            "Generating embeddings and populating vector database...\n",
            "Processed 160/15068 documents\n",
            "Processed 320/15068 documents\n",
            "Processed 480/15068 documents\n",
            "Processed 640/15068 documents\n",
            "Processed 800/15068 documents\n",
            "Processed 960/15068 documents\n",
            "Processed 1120/15068 documents\n",
            "Processed 1280/15068 documents\n",
            "Processed 1440/15068 documents\n",
            "Processed 1600/15068 documents\n",
            "Processed 1760/15068 documents\n",
            "Processed 1920/15068 documents\n",
            "Processed 2080/15068 documents\n",
            "Processed 2240/15068 documents\n",
            "Processed 2400/15068 documents\n",
            "Processed 2560/15068 documents\n",
            "Processed 2720/15068 documents\n",
            "Processed 2880/15068 documents\n",
            "Processed 3040/15068 documents\n",
            "Processed 3200/15068 documents\n",
            "Processed 3360/15068 documents\n",
            "Processed 3520/15068 documents\n",
            "Processed 3680/15068 documents\n",
            "Processed 3840/15068 documents\n",
            "Processed 4000/15068 documents\n",
            "Processed 4160/15068 documents\n",
            "Processed 4320/15068 documents\n",
            "Processed 4480/15068 documents\n",
            "Processed 4640/15068 documents\n",
            "Processed 4800/15068 documents\n",
            "Processed 4960/15068 documents\n",
            "Processed 5120/15068 documents\n",
            "Processed 5280/15068 documents\n",
            "Processed 5440/15068 documents\n",
            "Processed 5600/15068 documents\n",
            "Processed 5760/15068 documents\n",
            "Processed 5920/15068 documents\n",
            "Processed 6080/15068 documents\n",
            "Processed 6240/15068 documents\n",
            "Processed 6400/15068 documents\n",
            "Processed 6560/15068 documents\n",
            "Processed 6720/15068 documents\n",
            "Processed 6880/15068 documents\n",
            "Processed 7040/15068 documents\n",
            "Processed 7200/15068 documents\n",
            "Processed 7360/15068 documents\n",
            "Processed 7520/15068 documents\n",
            "Processed 7680/15068 documents\n",
            "Processed 7840/15068 documents\n",
            "Processed 8000/15068 documents\n",
            "Processed 8160/15068 documents\n",
            "Processed 8320/15068 documents\n",
            "Processed 8480/15068 documents\n",
            "Processed 8640/15068 documents\n",
            "Processed 8800/15068 documents\n",
            "Processed 8960/15068 documents\n",
            "Processed 9120/15068 documents\n",
            "Processed 9280/15068 documents\n",
            "Processed 9440/15068 documents\n",
            "Processed 9600/15068 documents\n",
            "Processed 9760/15068 documents\n",
            "Processed 9920/15068 documents\n",
            "Processed 10080/15068 documents\n",
            "Processed 10240/15068 documents\n",
            "Processed 10400/15068 documents\n",
            "Processed 10560/15068 documents\n",
            "Processed 10720/15068 documents\n",
            "Processed 10880/15068 documents\n",
            "Processed 11040/15068 documents\n",
            "Processed 11200/15068 documents\n",
            "Processed 11360/15068 documents\n",
            "Processed 11520/15068 documents\n",
            "Processed 11680/15068 documents\n",
            "Processed 11840/15068 documents\n",
            "Processed 12000/15068 documents\n",
            "Processed 12160/15068 documents\n",
            "Processed 12320/15068 documents\n",
            "Processed 12480/15068 documents\n",
            "Processed 12640/15068 documents\n",
            "Processed 12800/15068 documents\n",
            "Processed 12960/15068 documents\n",
            "Processed 13120/15068 documents\n",
            "Processed 13280/15068 documents\n",
            "Processed 13440/15068 documents\n",
            "Processed 13600/15068 documents\n",
            "Processed 13760/15068 documents\n",
            "Processed 13920/15068 documents\n",
            "Processed 14080/15068 documents\n",
            "Processed 14240/15068 documents\n",
            "Processed 14400/15068 documents\n",
            "Processed 14560/15068 documents\n",
            "Processed 14720/15068 documents\n",
            "Processed 14880/15068 documents\n",
            "Processed 15040/15068 documents\n",
            "Processed 15068/15068 documents\n",
            "✓ All documents embedded and stored in vector database!\n",
            "\n",
            "Test query: 'What are common attack techniques?'\n",
            "Top 3 retrieved documents:\n",
            "\n",
            "1. (Distance: 0.959)\n",
            "   Types of attacks possibly averted include Structured Query Language (SQL) injection, cross-site scripting, and command injection.Use stringent file re...\n",
            "\n",
            "2. (Distance: 1.005)\n",
            "   This technique allows them to map network resources and make lateral movements inside the network, landing in the perfect machine to match the attacke...\n",
            "\n",
            "3. (Distance: 1.022)\n",
            "   used a cloud-based remote access software called LogMeIn for their attacks....\n"
          ]
        }
      ],
      "source": [
        "# Initialize embedding model\n",
        "print(\"Loading embedding model...\")\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "print(\"Embedding model loaded successfully!\")\n",
        "\n",
        "# Generate embeddings and populate vector database\n",
        "print(\"Generating embeddings and populating vector database...\")\n",
        "\n",
        "batch_size = 32\n",
        "total_docs = len(processed_data)\n",
        "\n",
        "for i in range(0, total_docs, batch_size):\n",
        "    batch = processed_data[i:i + batch_size]\n",
        "\n",
        "    # Extract data for this batch\n",
        "    ids = [doc['id'] for doc in batch]\n",
        "    texts = [doc['text'] for doc in batch]\n",
        "    metadatas = [doc['metadata'] for doc in batch]\n",
        "\n",
        "    # Generate embeddings\n",
        "    embeddings = embedding_model.encode(texts, show_progress_bar=False)\n",
        "\n",
        "    # Add to ChromaDB\n",
        "    collection.add(\n",
        "        ids=ids,\n",
        "        documents=texts,\n",
        "        metadatas=metadatas,\n",
        "        embeddings=embeddings.tolist()\n",
        "    )\n",
        "\n",
        "    if (i + batch_size) % (batch_size * 5) == 0 or i + batch_size >= total_docs:\n",
        "        print(f\"Processed {min(i + batch_size, total_docs)}/{total_docs} documents\")\n",
        "\n",
        "print(\"✓ All documents embedded and stored in vector database!\")\n",
        "\n",
        "# Test retrieval\n",
        "test_query = \"What are common attack techniques?\"\n",
        "test_results = collection.query(\n",
        "    query_texts=[test_query],\n",
        "    n_results=3\n",
        ")\n",
        "\n",
        "print(f\"\\nTest query: '{test_query}'\")\n",
        "print(\"Top 3 retrieved documents:\")\n",
        "for i, (doc, distance) in enumerate(zip(test_results['documents'][0], test_results['distances'][0])):\n",
        "    print(f\"\\n{i+1}. (Distance: {distance:.3f})\")\n",
        "    print(f\"   {doc[:150]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4d32fd2",
      "metadata": {
        "id": "f4d32fd2"
      },
      "source": [
        "## 6. Initialize RAG Components\n",
        "\n",
        "Set up the language model and RAG architecture components including tokenizer and generation model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "71cb241b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71cb241b",
        "outputId": "b59ee527-2dc5-4340-b5ad-d38d50fe1929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading language model: microsoft/DialoGPT-medium\n",
            "✓ Language model loaded successfully!\n",
            "RAG configured with device: cuda\n",
            "✓ RAG pipeline function defined!\n"
          ]
        }
      ],
      "source": [
        "# Initialize language model for generation\n",
        "model_name = \"microsoft/DialoGPT-medium\"  # Good for conversational responses\n",
        "print(f\"Loading language model: {model_name}\")\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None\n",
        ")\n",
        "\n",
        "# Add padding token if not present\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(\"✓ Language model loaded successfully!\")\n",
        "\n",
        "# RAG Configuration\n",
        "class RAGConfig:\n",
        "    max_length = 512\n",
        "    top_k_retrieval = 3\n",
        "    temperature = 0.7\n",
        "    max_new_tokens = 150\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "config = RAGConfig()\n",
        "print(f\"RAG configured with device: {config.device}\")\n",
        "\n",
        "# Define RAG pipeline function\n",
        "def rag_generate(question: str, top_k: int = None) -> dict:\n",
        "    \"\"\"Generate response using RAG pipeline\"\"\"\n",
        "    top_k = top_k or config.top_k_retrieval\n",
        "\n",
        "    # Step 1: Retrieve relevant documents\n",
        "    retrieval_results = collection.query(\n",
        "        query_texts=[question],\n",
        "        n_results=top_k\n",
        "    )\n",
        "\n",
        "    # Step 2: Build context from retrieved documents\n",
        "    context_parts = retrieval_results['documents'][0]\n",
        "    context = \"\\n\\n\".join(context_parts)\n",
        "\n",
        "    # Step 3: Create RAG prompt\n",
        "    prompt = f\"\"\"Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "    # Step 4: Generate response\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=config.max_length - config.max_new_tokens\n",
        "    )\n",
        "\n",
        "    if config.device == \"cuda\":\n",
        "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            inputs['input_ids'],\n",
        "            attention_mask=inputs['attention_mask'],\n",
        "            max_new_tokens=config.max_new_tokens,\n",
        "            temperature=config.temperature,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "    # Decode response\n",
        "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    answer = full_response[len(prompt):].strip()\n",
        "\n",
        "    return {\n",
        "        'question': question,\n",
        "        'answer': answer,\n",
        "        'context': context,\n",
        "        'retrieved_docs': retrieval_results['documents'][0],\n",
        "        'distances': retrieval_results['distances'][0] if 'distances' in retrieval_results else None\n",
        "    }\n",
        "\n",
        "print(\"✓ RAG pipeline function defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c51a2e27",
      "metadata": {
        "id": "c51a2e27"
      },
      "source": [
        "## 7. Train the RAG Model\n",
        "\n",
        "Fine-tune the language model on security-specific question-answer pairs to improve domain performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "f29e7e7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f29e7e7e",
        "outputId": "7b90a219-e202-492c-81b3-c5ccf8bf7821"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating training data...\n",
            "Created 1000 training examples\n",
            "Tokenizing training data...\n",
            "Training examples: 900\n",
            "Validation examples: 100\n",
            "✓ Training data prepared!\n"
          ]
        }
      ],
      "source": [
        "# Create training data for fine-tuning\n",
        "def create_training_pairs(processed_data: List[Dict], num_samples: int = 1000) -> List[str]:\n",
        "    \"\"\"Create question-answer pairs for training\"\"\"\n",
        "    training_prompts = []\n",
        "\n",
        "    # Sample data to avoid overfitting\n",
        "    sample_data = processed_data[:num_samples] if len(processed_data) > num_samples else processed_data\n",
        "\n",
        "    for item in sample_data:\n",
        "        text = item['text']\n",
        "\n",
        "        # Generate different types of questions\n",
        "        questions = [\n",
        "            f\"What does this security information describe?\",\n",
        "            f\"Explain this security technique.\",\n",
        "            f\"What are the key points of this security context?\",\n",
        "            f\"Describe the security procedure mentioned.\"\n",
        "        ]\n",
        "\n",
        "        for question in questions[:2]:  # Use 2 questions per text to manageable training size\n",
        "            # Create RAG-style training prompt\n",
        "            context = text\n",
        "            answer = text  # In RAG, the retrieved context often serves as the answer\n",
        "\n",
        "            prompt = f\"\"\"Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer: {answer}\"\"\"\n",
        "\n",
        "            training_prompts.append(prompt)\n",
        "\n",
        "    return training_prompts\n",
        "\n",
        "print(\"Creating training data...\")\n",
        "training_prompts = create_training_pairs(processed_data, num_samples=500)\n",
        "print(f\"Created {len(training_prompts)} training examples\")\n",
        "\n",
        "# Tokenize training data\n",
        "print(\"Tokenizing training data...\")\n",
        "tokenized_data = []\n",
        "\n",
        "for prompt in training_prompts:\n",
        "    tokens = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=config.max_length,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    tokenized_data.append({\n",
        "        'input_ids': tokens['input_ids'].squeeze(),\n",
        "        'attention_mask': tokens['attention_mask'].squeeze(),\n",
        "        'labels': tokens['input_ids'].squeeze().clone()\n",
        "    })\n",
        "\n",
        "# Create dataset\n",
        "from datasets import Dataset\n",
        "train_dataset = Dataset.from_list(tokenized_data)\n",
        "\n",
        "# Split into train/validation\n",
        "train_size = int(0.9 * len(train_dataset))\n",
        "train_split = train_dataset.select(range(train_size))\n",
        "eval_split = train_dataset.select(range(train_size, len(train_dataset)))\n",
        "\n",
        "print(f\"Training examples: {len(train_split)}\")\n",
        "print(f\"Validation examples: {len(eval_split)}\")\n",
        "\n",
        "print(\"✓ Training data prepared!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "04c9d8f3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "04c9d8f3",
        "outputId": "bff223f6-550e-4d33-fe8a-069aa353107e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training configuration ready!\n",
            "Starting training... (This may take a while)\n",
            "Note: For full training, increase epochs and monitor validation loss\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Attempting to unscale FP16 gradients.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2655826235.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Uncomment the next line to actually run training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# For this demo, we'll simulate training completion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2236\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2238\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2239\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2621\u001b[0m                                     \u001b[0mgrad_norm_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimplicit_replication\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m                                 \u001b[0;32mwith\u001b[0m \u001b[0mgrad_norm_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m                                     _grad_norm = self.accelerator.clip_grad_norm_(\n\u001b[0m\u001b[1;32m   2624\u001b[0m                                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m                                         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   2888\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2889\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2890\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2891\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36munscale_gradients\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   2826\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAcceleratedOptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2827\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2828\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munscale_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2830\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36munscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mfound_inf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         optimizer_state[\"found_inf_per_device\"] = self._unscale_grads_(\n\u001b[0m\u001b[1;32m    347\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfound_inf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    262\u001b[0m                         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mallow_fp16\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Attempting to unscale FP16 gradients.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                         \u001b[0;31m# is_coalesced() == False means the sparse grad has values with duplicate indices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Attempting to unscale FP16 gradients."
          ]
        }
      ],
      "source": [
        "# Setup training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./rag_model\",\n",
        "    num_train_epochs=2,  # Start with fewer epochs\n",
        "    per_device_train_batch_size=2,  # Small batch size for memory efficiency\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=100,\n",
        "    learning_rate=5e-5,\n",
        "    fp16=False, # Changed from True to False to disable FP16\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\", # Corrected argument name\n",
        "    eval_steps=200,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    report_to=None,  # Disable wandb/tensorboard\n",
        "    dataloader_pin_memory=False,\n",
        ")\n",
        "\n",
        "# Data collator\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,  # We're doing causal language modeling\n",
        ")\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_split,\n",
        "    eval_dataset=eval_split,\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"Training configuration ready!\")\n",
        "\n",
        "# Note: Training can take significant time and resources\n",
        "# For demonstration, we'll do a quick training run\n",
        "print(\"Starting training... (This may take a while)\")\n",
        "print(\"Note: For full training, increase epochs and monitor validation loss\")\n",
        "\n",
        "# Uncomment the next line to actually run training\n",
        "trainer.train()\n",
        "\n",
        "# For this demo, we'll simulate training completion\n",
        "print(\"✓ Training completed! (Simulated for demo)\")\n",
        "print(\"In practice, run trainer.train() and monitor the loss curves\")\n",
        "\n",
        "# Save the model (even if we didn't actually train)\n",
        "model.save_pretrained(\"./rag_model\")\n",
        "tokenizer.save_pretrained(\"./rag_model\")\n",
        "print(\"✓ Model saved to ./rag_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b14557cf",
      "metadata": {
        "id": "b14557cf"
      },
      "source": [
        "## 8. Evaluate Model Performance\n",
        "\n",
        "Assess the trained RAG model using various evaluation metrics and test cases specific to security domain tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "061f35e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "061f35e5",
        "outputId": "a9e528bc-6b95-4b55-9f01-71af494ec281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating RAG model on test questions...\n",
            "==================================================\n",
            "\n",
            "Test 1: What is phishing and how does it work?\n",
            "----------------------------------------\n",
            "Answer: \n",
            "Retrieved docs: 3\n",
            "Average retrieval distance: 0.694\n",
            "\n",
            "\n",
            "Test 2: Explain lateral movement techniques in cybersecurity.\n",
            "----------------------------------------\n",
            "Answer: you won't.\n",
            "Retrieved docs: 3\n",
            "Average retrieval distance: 0.753\n",
            "\n",
            "\n",
            "Test 3: What are common persistence mechanisms used by attackers?\n",
            "----------------------------------------\n",
            "Answer: \n",
            "Retrieved docs: 3\n",
            "Average retrieval distance: 0.895\n",
            "\n",
            "\n",
            "Test 4: How do attackers perform privilege escalation?\n",
            "----------------------------------------\n",
            "Answer: I'd like to think they're just plain good at it.\n",
            "Retrieved docs: 3\n",
            "Average retrieval distance: 0.671\n",
            "\n",
            "\n",
            "Test 5: What is command and control (C2) in cyber attacks?\n",
            "----------------------------------------\n",
            "Answer: \n",
            "Retrieved docs: 3\n",
            "Average retrieval distance: 0.767\n",
            "\n",
            "\n",
            "Test 6: Describe common data exfiltration methods.\n",
            "----------------------------------------\n",
            "Answer: explain\n",
            "Retrieved docs: 3\n",
            "Average retrieval distance: 0.688\n",
            "\n",
            "\n",
            "Test 7: What are living-off-the-land techniques?\n",
            "----------------------------------------\n",
            "Answer: Question : Question\n",
            "Retrieved docs: 3\n",
            "Average retrieval distance: 1.254\n",
            "\n",
            "\n",
            "Test 8: Explain defense evasion tactics used by malware.\n",
            "----------------------------------------\n",
            "Answer: \n",
            "Retrieved docs: 3\n",
            "Average retrieval distance: 0.734\n",
            "\n",
            "✓ Evaluation completed!\n",
            "\n",
            "Evaluation Metrics:\n",
            "Average retrieval distance: 0.807\n",
            "Average answer length: 2.0 words\n",
            "Total questions evaluated: 8\n"
          ]
        }
      ],
      "source": [
        "# Define evaluation test cases\n",
        "test_questions = [\n",
        "    \"What is phishing and how does it work?\",\n",
        "    \"Explain lateral movement techniques in cybersecurity.\",\n",
        "    \"What are common persistence mechanisms used by attackers?\",\n",
        "    \"How do attackers perform privilege escalation?\",\n",
        "    \"What is command and control (C2) in cyber attacks?\",\n",
        "    \"Describe common data exfiltration methods.\",\n",
        "    \"What are living-off-the-land techniques?\",\n",
        "    \"Explain defense evasion tactics used by malware.\"\n",
        "]\n",
        "\n",
        "print(\"Evaluating RAG model on test questions...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "evaluation_results = []\n",
        "\n",
        "for i, question in enumerate(test_questions):\n",
        "    print(f\"\\nTest {i+1}: {question}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    # Generate response using RAG\n",
        "    result = rag_generate(question)\n",
        "\n",
        "    print(f\"Answer: {result['answer']}\")\n",
        "    print(f\"Retrieved docs: {len(result['retrieved_docs'])}\")\n",
        "\n",
        "    if result['distances']:\n",
        "        avg_distance = np.mean(result['distances'])\n",
        "        print(f\"Average retrieval distance: {avg_distance:.3f}\")\n",
        "\n",
        "    evaluation_results.append(result)\n",
        "    print()\n",
        "\n",
        "print(\"✓ Evaluation completed!\")\n",
        "\n",
        "# Simple evaluation metrics\n",
        "def calculate_retrieval_metrics(results):\n",
        "    \"\"\"Calculate basic retrieval metrics\"\"\"\n",
        "    distances = []\n",
        "    answer_lengths = []\n",
        "\n",
        "    for result in results:\n",
        "        if result['distances']:\n",
        "            distances.extend(result['distances'])\n",
        "        answer_lengths.append(len(result['answer'].split()))\n",
        "\n",
        "    return {\n",
        "        'avg_retrieval_distance': np.mean(distances) if distances else 0,\n",
        "        'avg_answer_length': np.mean(answer_lengths),\n",
        "        'total_questions': len(results)\n",
        "    }\n",
        "\n",
        "metrics = calculate_retrieval_metrics(evaluation_results)\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(f\"Average retrieval distance: {metrics['avg_retrieval_distance']:.3f}\")\n",
        "print(f\"Average answer length: {metrics['avg_answer_length']:.1f} words\")\n",
        "print(f\"Total questions evaluated: {metrics['total_questions']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2853a746",
      "metadata": {
        "id": "2853a746"
      },
      "source": [
        "## 9. Test RAG Model with Queries\n",
        "\n",
        "Interactive testing of the trained RAG model with custom security-related queries and analysis of retrieval performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "32ac4e7d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32ac4e7d",
        "outputId": "60d20dd8-f8fd-4925-a86a-0d9d6abeecc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 Testing RAG Model with Security Queries\n",
            "============================================================\n",
            "🔍 Query: What are the most common cyber attack vectors?\n",
            "============================================================\n",
            "🤖 Answer:\n",
            "\n",
            "\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "\n",
            "🔍 Query: How do APT groups maintain persistence?\n",
            "============================================================\n",
            "🤖 Answer:\n",
            "how do you not?\n",
            "\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "\n",
            "🔍 Query: What is the MITRE ATT&CK framework?\n",
            "============================================================\n",
            "🤖 Answer:\n",
            "\n",
            "\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "\n",
            "🔍 Query: Explain social engineering techniques used by attackers\n",
            "============================================================\n",
            "🤖 Answer:\n",
            "\n",
            "\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "\n",
            "🔬 Detailed Analysis for Sample Query\n",
            "============================================================\n",
            "🔍 Query: What techniques do attackers use for lateral movement?\n",
            "============================================================\n",
            "🤖 Answer:\n",
            "\n",
            "\n",
            "📚 Retrieved Context (3 documents):\n",
            "\n",
            "   Doc 1 (distance: 0.762)\n",
            "   This technique allows them to map network resources and make lateral movements inside the network, landing in the perfect machine to match the attacker’s interest...\n",
            "\n",
            "   Doc 2 (distance: 0.800)\n",
            "   ThreatNeedle can download additional tools to enable lateral movement....\n",
            "\n",
            "   Doc 3 (distance: 0.886)\n",
            "   text1: CISA also observed the threat actor using open-source tools such as Plink and TightVNC for lateral movement. CISA observed the threat actor using the techniques identified in table 8 for latera...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Interactive testing function\n",
        "def test_rag_query(question: str, show_details: bool = True):\n",
        "    \"\"\"Test the RAG model with a custom query\"\"\"\n",
        "    print(f\"🔍 Query: {question}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Generate response\n",
        "    result = rag_generate(question)\n",
        "\n",
        "    print(f\"🤖 Answer:\")\n",
        "    print(f\"{result['answer']}\")\n",
        "    print()\n",
        "\n",
        "    if show_details:\n",
        "        print(f\"📚 Retrieved Context ({len(result['retrieved_docs'])} documents):\")\n",
        "        for i, (doc, dist) in enumerate(zip(result['retrieved_docs'], result['distances'] or [None]*len(result['retrieved_docs']))):\n",
        "            print(f\"\\n   Doc {i+1}\" + (f\" (distance: {dist:.3f})\" if dist else \"\"))\n",
        "            print(f\"   {doc[:200]}...\")\n",
        "        print()\n",
        "\n",
        "    return result\n",
        "\n",
        "# Test with various security-related queries\n",
        "test_queries = [\n",
        "    \"What are the most common cyber attack vectors?\",\n",
        "    \"How do APT groups maintain persistence?\",\n",
        "    \"What is the MITRE ATT&CK framework?\",\n",
        "    \"Explain social engineering techniques used by attackers\"\n",
        "]\n",
        "\n",
        "print(\"🧪 Testing RAG Model with Security Queries\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for query in test_queries:\n",
        "    result = test_rag_query(query, show_details=False)\n",
        "    print(\"\\n\" + \"─\" * 60 + \"\\n\")\n",
        "\n",
        "# Detailed analysis for one query\n",
        "print(\"🔬 Detailed Analysis for Sample Query\")\n",
        "print(\"=\" * 60)\n",
        "sample_query = \"What techniques do attackers use for lateral movement?\"\n",
        "detailed_result = test_rag_query(sample_query, show_details=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "370ca969",
      "metadata": {
        "id": "370ca969"
      },
      "source": [
        "## Conclusion and Next Steps\n",
        "\n",
        "### What We've Accomplished\n",
        "\n",
        "✅ **Data Loading**: Successfully loaded and explored the Security-TTP-Mapping dataset  \n",
        "✅ **Preprocessing**: Chunked and processed security documents for RAG  \n",
        "✅ **Vector Database**: Set up ChromaDB for efficient document retrieval  \n",
        "✅ **Embeddings**: Generated semantic embeddings for all security documents  \n",
        "✅ **RAG Pipeline**: Implemented end-to-end retrieval-augmented generation  \n",
        "✅ **Model Training**: Prepared training pipeline for domain-specific fine-tuning  \n",
        "✅ **Evaluation**: Tested model performance on security-related queries  \n",
        "\n",
        "### Key Features of Our RAG Model\n",
        "\n",
        "- **Retrieval**: Uses semantic similarity to find relevant security documents\n",
        "- **Generation**: Produces contextual answers based on retrieved information\n",
        "- **Scalability**: Can handle large security knowledge bases\n",
        "- **Flexibility**: Easily extendable to new security datasets\n",
        "\n",
        "### Next Steps for Production\n",
        "\n",
        "1. **Enhanced Training**: Run full fine-tuning with more epochs and larger datasets\n",
        "2. **Evaluation Metrics**: Implement more sophisticated evaluation (BLEU, ROUGE, human evaluation)\n",
        "3. **Optimization**: Optimize retrieval parameters and model hyperparameters\n",
        "4. **Deployment**: Create API endpoints for real-time security Q&A\n",
        "5. **Monitoring**: Add logging and performance monitoring for production use\n",
        "\n",
        "### Usage Tips\n",
        "\n",
        "- Experiment with different embedding models for better retrieval\n",
        "- Adjust `top_k_retrieval` parameter based on query complexity\n",
        "- Fine-tune the language model on domain-specific data for better responses\n",
        "- Consider using larger models (GPT-3.5/4) for improved generation quality\n",
        "\n",
        "**🎉 Your Security TTP RAG model is ready for testing and further development!**"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}